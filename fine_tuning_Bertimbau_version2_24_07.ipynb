{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU_ldpdMEMbw",
        "outputId": "8d6ae346-1816-4ea9-8b61-2f9e8f4840e1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbstripout\n",
        "!nbstripout /content/drive/MyDrive/2025/tcc-final/fine_tuning_Bertimbau_version2_24_07.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BkkHsJY5z-5",
        "outputId": "a7661ae8-7d77-4296-805e-891fa9b1fc92"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbstripout in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from nbstripout) (5.10.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (4.25.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (5.8.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.26.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->nbstripout) (4.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->nbstripout) (4.14.1)\n",
            "Could not strip '/content/drive/MyDrive/2025/tcc-final/fine_tuning_Bertimbau_version2_24_07.ipynb': file not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feHRGfqdONsa",
        "outputId": "42bb23f1-d5ab-4d89-a7f1-5274ec9677c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers evaluate accelerate\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback # Importa explicitamente para o callback\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# 1. Carregar Dataset Processado**"
      ],
      "metadata": {
        "id": "qkRUcLU1PHke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui além de carregar os dataset, também é feita a divisão dos dados em treino, validação e testes"
      ],
      "metadata": {
        "id": "PLMoitZ6Ylnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar dataset já processado\n",
        "file_path = '/content/drive/MyDrive/2025/tcc-final/denuncias_balanceadas.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "print(f\"Dataset carregado. Total de registros: {len(df)}\")\n",
        "print(\"Primeiras 5 linhas do dataset:\")\n",
        "print(df.head())\n",
        "print(\"\\nInformações do dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "\"\"\"# Dividir em treino (70%), validação (15%) e teste (15%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['classe'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['classe'], random_state=42)\n",
        "\n",
        "print(f\"Treino: {len(train_df)}, Validação: {len(val_df)}, Teste: {len(test_df)}\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "3O3x60E3PHtT",
        "outputId": "0a820503-cc1b-4eec-af7e-eaa0eb700f16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset carregado. Total de registros: 169\n",
            "Primeiras 5 linhas do dataset:\n",
            "                                               texto             classe\n",
            "0  Segundo informações da [VÍTIMA], na sexta, a m...  invasao_domicilio\n",
            "1  Policiais arrombaram a casa, quebraram sapatei...  invasao_domicilio\n",
            "2  [VÍTIMA] relata que sua vizinha [VÍTIMA] teve ...  invasao_domicilio\n",
            "3  [VÍTIMA] relata que abriram o portão de sua ca...  invasao_domicilio\n",
            "4  [VÍTIMA] teve a casa invadida por policiais do...  invasao_domicilio\n",
            "\n",
            "Informações do dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 169 entries, 0 to 168\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   texto   169 non-null    object\n",
            " 1   classe  169 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 2.8+ KB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Dividir em treino (70%), validação (15%) e teste (15%)\\ntrain_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\\'classe\\'], random_state=42)\\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\\'classe\\'], random_state=42)\\n\\nprint(f\"Treino: {len(train_df)}, Validação: {len(val_df)}, Teste: {len(test_df)}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JIeftoByY356"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Tokenização com BERTimbau**"
      ],
      "metadata": {
        "id": "DE3WAEgOPYHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparação os dados textuais (as denúncias) para serem compreendidos pelo modelo BERTimbau, convertendo o texto bruto em um formato numérico que o modelo pode processar.\n",
        "*   AutoTokenizer é uma classe da biblioteca transformers que automaticamente seleciona a classe de tokenizador correta com base no nome do modelo.\n",
        "\n",
        "*   Este tokenizador sabe como dividir o texto em \"tokens\" (palavras ou subpalavras), converter esses tokens em IDs numéricos (que o modelo entende), e adicionar tokens especiais que o BERT espera (como [CLS] e [SEP]).\n",
        "* Além disso, a função tokenize_function receberá um \"batch\" (lote) de dados como entrada.\n",
        "\n"
      ],
      "metadata": {
        "id": "AoC0YTBEYuA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    # 'padding='longest'' preenche cada sequência para o tamanho da sequência mais longa no lote atual,\n",
        "    # em vez de um max_length fixo, o que é mais eficiente.\n",
        "    # 'truncation=True' ainda corta sequências maiores que max_length.\n",
        "    return tokenizer(batch[\"texto\"], truncation=True, max_length=512, padding='longest', return_tensors='pt')\n",
        "\n",
        "# Não tokenizamos o dataset completo aqui para K-Fold, faremos isso dentro do loop.\n",
        "# As variáveis train_encodings, val_encodings, test_encodings serão criadas dentro do loop K-Fold.\n",
        "\n",
        "# Mapeamento de Labels: Essencial para o modelo e métricas\n",
        "id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# Custom Dataset (para trabalhar com tensores PyTorch diretamente)\n",
        "class CustomDataset(Dataset): # Certifique-se que 'Dataset' foi importado corretamente\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings # Agora será um dicionário de tensores (input_ids, attention_mask)\n",
        "        self.labels = labels       # Agora será uma lista de labels (ou Series com índice reiniciado)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        # val[idx] funciona diretamente em tensores ou listas de tensores.\n",
        "        # labels.iloc[idx] é removido pois labels será uma lista/Series com índice reiniciado.\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Acesso direto pelo índice\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Data Collator para padding dinâmico durante o treinamento\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\"\"\"# Função de tokenização\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch[\"texto\"], truncation=True,max_length=512, padding=True,return_tensors='pt')\n",
        "\n",
        "# Tokenizar os DataFrames diretamente (sem usar datasets.Dataset)\n",
        "train_encodings = tokenize_function(train_df.to_dict('list'))\n",
        "val_encodings = tokenize_function(val_df.to_dict('list'))\n",
        "test_encodings = tokenize_function(test_df.to_dict('list'))\"\"\""
      ],
      "metadata": {
        "id": "sql7RSXMPd1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "52153a52-383a-4d4b-be76-4c81177b4e70"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Função de tokenização\\ndef tokenize_function(batch):\\n    return tokenizer(batch[\"texto\"], truncation=True,max_length=512, padding=True,return_tensors=\\'pt\\')\\n\\n# Tokenizar os DataFrames diretamente (sem usar datasets.Dataset)\\ntrain_encodings = tokenize_function(train_df.to_dict(\\'list\\'))\\nval_encodings = tokenize_function(val_df.to_dict(\\'list\\'))\\ntest_encodings = tokenize_function(test_df.to_dict(\\'list\\'))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Configurar o Modelo e congelamento**"
      ],
      "metadata": {
        "id": "U0mzN3JPP-s4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializar o modelo pré-treinado BERTimbau para uma tarefa de classificação de sequências e, crucialmente, configurar quais partes desse modelo serão treinadas (ajustadas) e quais permanecerão \"congeladas\".\n",
        "* Mapeamento de Labels\n",
        "* Carregamento e Configuração do Modelo de Classificação\n",
        "* Congelamento de Camadas do BERT\n",
        " * for name, param in model.named_parameters():: Inicia um loop que itera sobre todos os parâmetros (pesos e vieses) do modelo. Para cada parâmetro, ele fornece seu name (nome da camada a que pertence) e o próprio param (o tensor do parâmetro).\n",
        "\n",
        " * if 'classifier' not in name and 'pooler' not in name:: Esta é a condição principal para o congelamento.\n",
        "\n",
        " * Ele verifica se o name do parâmetro não contém a string 'classifier' e não contém a string 'pooler'.\n",
        "\n",
        " * 'classifier': Refere-se aos parâmetros da cabeça de classificação que foi adicionada no topo do modelo BERT (a parte que faz a previsão das 2 classes).\n",
        "\n",
        " * 'pooler': Refere-se aos parâmetros da camada \"pooler\" do BERT. O pooler geralmente é usado para obter uma representação vetorial de tamanho fixo para toda a sequência de entrada. Para tarefas de classificação de sequência, o BERT frequentemente usa a saída do token [CLS] (o primeiro token) que passa pelo pooler."
      ],
      "metadata": {
        "id": "SoExoeSAZrQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_path,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Congelar camadas do BERT (exceto pooler e classificador)\n",
        "#for name, param in model.named_parameters():\n",
        "#    if 'classifier' not in name and 'pooler' not in name:\n",
        "#        param.requires_grad = False\n",
        "\n",
        "# Congelar todas as camadas da base BERT\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Garantir que a cabeça de classificação seja treinável\n",
        "# (Ela já é treinável por padrão se não for explicitamente congelada)\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Opcional: Imprimir o número de parâmetros treináveis para verificar\n",
        "print(f\"Número de parâmetros treináveis: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "tkQIL_4sYTMq",
        "outputId": "793145ec-52c4-44ca-b949-7d72ebca54fd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\\nlabel2id = {v: k for k, v in id2label.items()}\\n\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    model_path,\\n    num_labels=2,\\n    id2label=id2label,\\n    label2id=label2id\\n)\\n\\n# Congelar camadas do BERT (exceto pooler e classificador)\\n#for name, param in model.named_parameters():\\n#    if \\'classifier\\' not in name and \\'pooler\\' not in name:\\n#        param.requires_grad = False\\n\\n# Congelar todas as camadas da base BERT\\nfor param in model.bert.parameters():\\n    param.requires_grad = False\\n\\n# Garantir que a cabeça de classificação seja treinável\\n# (Ela já é treinável por padrão se não for explicitamente congelada)\\nfor param in model.classifier.parameters():\\n    param.requires_grad = True\\n\\n# Opcional: Imprimir o número de parâmetros treináveis para verificar\\nprint(f\"Número de parâmetros treináveis: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. MÉTRICAS AVANÇADAS (Acurácia + AUC-ROC)**"
      ],
      "metadata": {
        "id": "2S-nz2HQZ0CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "auc_score = evaluate.load(\"roc_auc\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    try:\n",
        "        predictions, labels = eval_pred\n",
        "\n",
        "        # Converter logits em probabilidades usando softmax (para AUC-ROC)\n",
        "        # np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True) é o softmax numérico\n",
        "        probabilities = np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)) / np.sum(np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)), axis=-1, keepdims=True)\n",
        "        positive_class_probs = probabilities[:, 1] # Probabilidade da classe positiva (ID 1)\n",
        "\n",
        "        # Calcular métricas básicas\n",
        "        preds = np.argmax(predictions, axis=1) # Classe prevista (ID da maior probabilidade)\n",
        "        acc = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "        auc = auc_score.compute(prediction_scores=positive_class_probs, references=labels)[\"roc_auc\"]\n",
        "\n",
        "        # Calcular F1 para cada classe (average=None)\n",
        "        f1_results = f1_metric.compute(\n",
        "            predictions=preds,\n",
        "            references=labels,\n",
        "            average=None, # Calcula F1 para cada classe individualmente\n",
        "            labels=[label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]] # IDs das classes\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": round(acc, 4),\n",
        "            \"auc\": round(auc, 4),\n",
        "            \"f1_invasao\": round(f1_results[\"f1\"][0], 4),\n",
        "            \"f1_violencia\": round(f1_results[\"f1\"][1], 4)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no cálculo de métricas: {str(e)}\")\n",
        "        # Retorna 0s ou NaN em caso de erro para não quebrar o processo, mas indica problema\n",
        "        return {\"accuracy\": 0.0, \"auc\": 0.0, \"f1_invasao\": 0.0, \"f1_violencia\": 0.0}"
      ],
      "metadata": {
        "id": "F-gctrIbZ1Pv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. TREINAMENTO COM K-FOLD CROSS-VALIDATION E EARLY STOPPING**"
      ],
      "metadata": {
        "id": "7fmwNJA-Z5DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parâmetros de Treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bertimbau-denuncias-cv\", # Diretório de saída para K-Fold\n",
        "\n",
        "    # Configurações básicas\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    #learning_rate=2e-5,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=30, # Aumentado para permitir que Early Stopping atue\n",
        "    # Anteriormente: num_train_epochs=3, o que não dava espaço para Early Stopping\n",
        "\n",
        "    # Estratégias\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    # Configurações de avaliação e salvamento\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\", # Monitorar a perda de validação\n",
        "    greater_is_better=False, # Menor perda é melhor\n",
        "\n",
        "    # Logging\n",
        "    logging_dir=\"./logs-cv\",\n",
        "    logging_steps=10, # Diminuído para ver logs mais frequentemente (era 100)\n",
        "    disable_tqdm=False,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # Otimizações\n",
        "    fp16=True, # Treinamento com precisão mista\n",
        "    seed=42, # Semente para reprodutibilidade\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    # Parâmetros específicos da v4.53.2\n",
        "    remove_unused_columns=True,\n",
        "    label_names=[\"labels\"]\n",
        ")\n",
        "\n",
        "# Preparar dados para K-Fold\n",
        "all_texts = df['texto'].tolist()\n",
        "all_labels_mapped = df['classe'].map(label2id).tolist() # Labels já mapeadas para IDs numéricos\n",
        "\n",
        "N_SPLITS = 5 # Número de folds. Para 150 registros, 5 folds é razoável.\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "\n",
        "# Lista para armazenar as métricas de teste de cada fold\n",
        "all_fold_test_metrics = []\n",
        "\n",
        "print(f\"\\n🚀 Iniciando Treinamento com K-Fold Cross-Validation ({N_SPLITS} folds)...\\n\")\n",
        "\n",
        "# Loop através dos folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(all_texts, all_labels_mapped)):\n",
        "    print(f\"\\n==================== INICIANDO FOLD {fold+1}/{N_SPLITS} ====================\")\n",
        "\n",
        "    # Dividir os dados para o fold atual\n",
        "    train_fold_texts = [all_texts[i] for i in train_index]\n",
        "    train_fold_labels = [all_labels_mapped[i] for i in train_index]\n",
        "\n",
        "    test_fold_texts = [all_texts[i] for i in test_index]\n",
        "    test_fold_labels = [all_labels_mapped[i] for i in test_index]\n",
        "\n",
        "    # Tokenizar os dados para o fold atual\n",
        "    # Note: tokenize_function já retorna um dicionário de tensores PyTorch.\n",
        "    train_fold_encodings = tokenize_function({\"texto\": train_fold_texts})\n",
        "    test_fold_encodings = tokenize_function({\"texto\": test_fold_texts})\n",
        "\n",
        "    # Criar CustomDatasets para o fold atual\n",
        "    # Passamos os encodings (dicionário de tensores) e as labels (lista) diretamente\n",
        "    # O CustomDataset agora espera isso e acessa por índice posicional.\n",
        "    train_dataset = CustomDataset(\n",
        "        train_fold_encodings, # Já é um dicionário de tensores\n",
        "        train_fold_labels     # Já é uma lista\n",
        "    )\n",
        "    eval_dataset_for_trainer = CustomDataset(\n",
        "        test_fold_encodings,\n",
        "        test_fold_labels\n",
        "    )\n",
        "\n",
        "    # Re-instanciar o modelo para cada fold para garantir pesos iniciais limpos\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_path,\n",
        "        num_labels=2,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    # Adicione ESTE BLOCO de CONGELAMENTO AQUI\n",
        "    # Congelar camadas do BERT (exceto pooler e classificador)\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'classifier' not in name and 'pooler' not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Opcional: Imprimir o número de parâmetros treináveis para verificar\n",
        "    print(f\"  Número de parâmetros treináveis no FOLD {fold+1}: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "    # ===============================================\n",
        "\n",
        "    # Re-instanciar o Trainer para cada fold\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset_for_trainer, # Conjunto de teste/validação do fold\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], # Aumentar paciência para 5 épocas\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=data_collator # Passado explicitamente\n",
        "    )\n",
        "\n",
        "    # Treinar o modelo para o fold atual\n",
        "    try:\n",
        "        train_result = trainer.train()\n",
        "        print(f\"📊 Métricas finais de treino para FOLD {fold+1}:\")\n",
        "        print(f\"Loss: {train_result.metrics['train_loss']:.4f}\")\n",
        "        print(f\"Tempo total: {train_result.metrics['train_runtime']:.2f}s\")\n",
        "\n",
        "        # Avaliar no conjunto de teste/validação do fold\n",
        "        print(f\"🧪 Avaliação no conjunto de teste/validação do FOLD {fold+1}...\")\n",
        "        fold_test_metrics = trainer.evaluate(eval_dataset_for_trainer)\n",
        "        all_fold_test_metrics.append(fold_test_metrics)\n",
        "        print(f\"Resultados do FOLD {fold+1}: {fold_test_metrics}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"❌ Erro durante o treinamento do FOLD {fold+1}:\")\n",
        "        print(traceback.format_exc())\n",
        "        # Em caso de erro, adiciona métricas vazias ou NaN para não parar a média final\n",
        "        all_fold_test_metrics.append({\n",
        "            'eval_loss': float('nan'),\n",
        "            'eval_accuracy': 0.0,\n",
        "            'eval_auc': 0.0,\n",
        "            'eval_f1_invasao': 0.0,\n",
        "            'eval_f1_violencia': 0.0,\n",
        "            'eval_runtime': float('nan'), # Inclui outras métricas relevantes\n",
        "            'eval_samples_per_second': 0.0,\n",
        "            'eval_steps_per_second': 0.0\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lu5KpVfWaGA3",
        "outputId": "3edd90e2-bc30-4398-93e4-c2ac3e53e12e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Iniciando Treinamento com K-Fold Cross-Validation (5 folds)...\n",
            "\n",
            "\n",
            "==================== INICIANDO FOLD 1/5 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Número de parâmetros treináveis no FOLD 1: 592130\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='425' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [425/510 03:21 < 00:40, 2.10 it/s, Epoch 25/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auc</th>\n",
              "      <th>F1 Invasao</th>\n",
              "      <th>F1 Violencia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.697100</td>\n",
              "      <td>0.658275</td>\n",
              "      <td>0.676500</td>\n",
              "      <td>0.788900</td>\n",
              "      <td>0.521700</td>\n",
              "      <td>0.755600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.653500</td>\n",
              "      <td>0.622393</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.802800</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.604000</td>\n",
              "      <td>0.594123</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.556300</td>\n",
              "      <td>0.572582</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.816600</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.544000</td>\n",
              "      <td>0.552361</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.820100</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.500100</td>\n",
              "      <td>0.532729</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.495600</td>\n",
              "      <td>0.524172</td>\n",
              "      <td>0.705900</td>\n",
              "      <td>0.847800</td>\n",
              "      <td>0.666700</td>\n",
              "      <td>0.736800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.509700</td>\n",
              "      <td>0.509753</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.868500</td>\n",
              "      <td>0.689700</td>\n",
              "      <td>0.769200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.466300</td>\n",
              "      <td>0.486396</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.872000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.460700</td>\n",
              "      <td>0.475388</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.872000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.445000</td>\n",
              "      <td>0.466668</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.885800</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.416100</td>\n",
              "      <td>0.460618</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.382800</td>\n",
              "      <td>0.441810</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.892700</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.414800</td>\n",
              "      <td>0.434702</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.892700</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.429400</td>\n",
              "      <td>0.426927</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.892700</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.378300</td>\n",
              "      <td>0.422098</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.899700</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.340500</td>\n",
              "      <td>0.418052</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.899700</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.326000</td>\n",
              "      <td>0.410985</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.899700</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.355300</td>\n",
              "      <td>0.410094</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.899700</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.357500</td>\n",
              "      <td>0.404703</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.343000</td>\n",
              "      <td>0.398663</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.301200</td>\n",
              "      <td>0.393645</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.329300</td>\n",
              "      <td>0.394264</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.352700</td>\n",
              "      <td>0.395267</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.359900</td>\n",
              "      <td>0.394307</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Métricas finais de treino para FOLD 1:\n",
            "Loss: 0.4390\n",
            "Tempo total: 201.36s\n",
            "🧪 Avaliação no conjunto de teste/validação do FOLD 1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados do FOLD 1: {'eval_loss': 0.39364489912986755, 'eval_accuracy': 0.7941, 'eval_auc': 0.9031, 'eval_f1_invasao': 0.7879, 'eval_f1_violencia': 0.8, 'eval_runtime': 0.2193, 'eval_samples_per_second': 155.043, 'eval_steps_per_second': 22.8, 'epoch': 25.0}\n",
            "\n",
            "==================== INICIANDO FOLD 2/5 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Número de parâmetros treináveis no FOLD 2: 592130\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='510' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [510/510 04:47, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auc</th>\n",
              "      <th>F1 Invasao</th>\n",
              "      <th>F1 Violencia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.696800</td>\n",
              "      <td>0.611522</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.920400</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.656900</td>\n",
              "      <td>0.584760</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.917000</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.609000</td>\n",
              "      <td>0.559455</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.917000</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.572800</td>\n",
              "      <td>0.536398</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.913500</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.566500</td>\n",
              "      <td>0.513586</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.923900</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.538100</td>\n",
              "      <td>0.500481</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.504800</td>\n",
              "      <td>0.485470</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.930800</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.475600</td>\n",
              "      <td>0.473313</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.468800</td>\n",
              "      <td>0.459934</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.445200</td>\n",
              "      <td>0.449843</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.423000</td>\n",
              "      <td>0.441539</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.441000</td>\n",
              "      <td>0.439250</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.758600</td>\n",
              "      <td>0.820500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.393500</td>\n",
              "      <td>0.427611</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.404500</td>\n",
              "      <td>0.417154</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.394800</td>\n",
              "      <td>0.416982</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.758600</td>\n",
              "      <td>0.820500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.356200</td>\n",
              "      <td>0.406245</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.384000</td>\n",
              "      <td>0.399959</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.381900</td>\n",
              "      <td>0.393313</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.360600</td>\n",
              "      <td>0.392329</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.842100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.345600</td>\n",
              "      <td>0.391322</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.842100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.297300</td>\n",
              "      <td>0.386121</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.934300</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.842100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.316700</td>\n",
              "      <td>0.382163</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.358400</td>\n",
              "      <td>0.383134</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.758600</td>\n",
              "      <td>0.820500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.341500</td>\n",
              "      <td>0.380270</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.842100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.340700</td>\n",
              "      <td>0.376665</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.327500</td>\n",
              "      <td>0.374477</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.321600</td>\n",
              "      <td>0.374295</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.297000</td>\n",
              "      <td>0.373410</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.354100</td>\n",
              "      <td>0.372965</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.325500</td>\n",
              "      <td>0.372869</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.937700</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Métricas finais de treino para FOLD 2:\n",
            "Loss: 0.4235\n",
            "Tempo total: 287.39s\n",
            "🧪 Avaliação no conjunto de teste/validação do FOLD 2...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados do FOLD 2: {'eval_loss': 0.3728691637516022, 'eval_accuracy': 0.8529, 'eval_auc': 0.9377, 'eval_f1_invasao': 0.8387, 'eval_f1_violencia': 0.8649, 'eval_runtime': 0.2739, 'eval_samples_per_second': 124.119, 'eval_steps_per_second': 18.253, 'epoch': 30.0}\n",
            "\n",
            "==================== INICIANDO FOLD 3/5 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Número de parâmetros treináveis no FOLD 3: 592130\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='510' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [510/510 04:18, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auc</th>\n",
              "      <th>F1 Invasao</th>\n",
              "      <th>F1 Violencia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.682900</td>\n",
              "      <td>0.654993</td>\n",
              "      <td>0.588200</td>\n",
              "      <td>0.799300</td>\n",
              "      <td>0.416700</td>\n",
              "      <td>0.681800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.638600</td>\n",
              "      <td>0.626458</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.799300</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.599200</td>\n",
              "      <td>0.607092</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.785500</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.553700</td>\n",
              "      <td>0.593387</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.802800</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.563100</td>\n",
              "      <td>0.579820</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.813100</td>\n",
              "      <td>0.733300</td>\n",
              "      <td>0.789500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.495800</td>\n",
              "      <td>0.569049</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.809700</td>\n",
              "      <td>0.689700</td>\n",
              "      <td>0.769200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.492100</td>\n",
              "      <td>0.561173</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.809700</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.433200</td>\n",
              "      <td>0.556300</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.802800</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.458600</td>\n",
              "      <td>0.547087</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.813100</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.429000</td>\n",
              "      <td>0.542659</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.816600</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.415400</td>\n",
              "      <td>0.536793</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.813100</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.401200</td>\n",
              "      <td>0.531675</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.818300</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.384200</td>\n",
              "      <td>0.524877</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.830400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.382200</td>\n",
              "      <td>0.520802</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.833900</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.413700</td>\n",
              "      <td>0.518747</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.830400</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.374000</td>\n",
              "      <td>0.514563</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.840800</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.344700</td>\n",
              "      <td>0.511310</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.840800</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.361000</td>\n",
              "      <td>0.509597</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.840800</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.261000</td>\n",
              "      <td>0.507631</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.319300</td>\n",
              "      <td>0.505747</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.840800</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.314200</td>\n",
              "      <td>0.502761</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.329800</td>\n",
              "      <td>0.503834</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.837400</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.343700</td>\n",
              "      <td>0.503241</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.837400</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.358500</td>\n",
              "      <td>0.502378</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.837400</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.319500</td>\n",
              "      <td>0.500865</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.840800</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.282300</td>\n",
              "      <td>0.499576</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.709700</td>\n",
              "      <td>0.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.326800</td>\n",
              "      <td>0.498735</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.330200</td>\n",
              "      <td>0.498416</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.269600</td>\n",
              "      <td>0.498213</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.340300</td>\n",
              "      <td>0.498147</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Métricas finais de treino para FOLD 3:\n",
            "Loss: 0.4029\n",
            "Tempo total: 258.21s\n",
            "🧪 Avaliação no conjunto de teste/validação do FOLD 3...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados do FOLD 3: {'eval_loss': 0.4981469511985779, 'eval_accuracy': 0.7647, 'eval_auc': 0.8443, 'eval_f1_invasao': 0.75, 'eval_f1_violencia': 0.7778, 'eval_runtime': 0.3433, 'eval_samples_per_second': 99.029, 'eval_steps_per_second': 14.563, 'epoch': 30.0}\n",
            "\n",
            "==================== INICIANDO FOLD 4/5 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Número de parâmetros treináveis no FOLD 4: 592130\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='510' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [510/510 04:01, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auc</th>\n",
              "      <th>F1 Invasao</th>\n",
              "      <th>F1 Violencia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.679200</td>\n",
              "      <td>0.640323</td>\n",
              "      <td>0.676500</td>\n",
              "      <td>0.865100</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.744200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.643200</td>\n",
              "      <td>0.609698</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.756800</td>\n",
              "      <td>0.709700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.594900</td>\n",
              "      <td>0.579410</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.581100</td>\n",
              "      <td>0.561997</td>\n",
              "      <td>0.705900</td>\n",
              "      <td>0.878900</td>\n",
              "      <td>0.722200</td>\n",
              "      <td>0.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.535500</td>\n",
              "      <td>0.543856</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.865100</td>\n",
              "      <td>0.742900</td>\n",
              "      <td>0.727300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.494000</td>\n",
              "      <td>0.529785</td>\n",
              "      <td>0.735300</td>\n",
              "      <td>0.861600</td>\n",
              "      <td>0.742900</td>\n",
              "      <td>0.727300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.503200</td>\n",
              "      <td>0.508268</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.878900</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.434300</td>\n",
              "      <td>0.494284</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.875400</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.470200</td>\n",
              "      <td>0.482079</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.889300</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.474100</td>\n",
              "      <td>0.470576</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.892700</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.422500</td>\n",
              "      <td>0.459049</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.903100</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.432700</td>\n",
              "      <td>0.449694</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.370500</td>\n",
              "      <td>0.439488</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.913500</td>\n",
              "      <td>0.838700</td>\n",
              "      <td>0.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.412100</td>\n",
              "      <td>0.431948</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.913500</td>\n",
              "      <td>0.848500</td>\n",
              "      <td>0.857100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.378700</td>\n",
              "      <td>0.420507</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.913500</td>\n",
              "      <td>0.866700</td>\n",
              "      <td>0.894700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>0.414948</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.913500</td>\n",
              "      <td>0.866700</td>\n",
              "      <td>0.894700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.384000</td>\n",
              "      <td>0.406670</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.920400</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.888900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.354800</td>\n",
              "      <td>0.406461</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.920400</td>\n",
              "      <td>0.848500</td>\n",
              "      <td>0.857100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.358400</td>\n",
              "      <td>0.396921</td>\n",
              "      <td>0.911800</td>\n",
              "      <td>0.923900</td>\n",
              "      <td>0.903200</td>\n",
              "      <td>0.918900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.320600</td>\n",
              "      <td>0.389760</td>\n",
              "      <td>0.911800</td>\n",
              "      <td>0.930800</td>\n",
              "      <td>0.903200</td>\n",
              "      <td>0.918900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.319500</td>\n",
              "      <td>0.386332</td>\n",
              "      <td>0.911800</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.903200</td>\n",
              "      <td>0.918900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.334300</td>\n",
              "      <td>0.384116</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.376200</td>\n",
              "      <td>0.383755</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.333500</td>\n",
              "      <td>0.379866</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.297600</td>\n",
              "      <td>0.377099</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.324700</td>\n",
              "      <td>0.375720</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.302500</td>\n",
              "      <td>0.373989</td>\n",
              "      <td>0.911800</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.903200</td>\n",
              "      <td>0.918900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.278100</td>\n",
              "      <td>0.373308</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.373018</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.333700</td>\n",
              "      <td>0.372953</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.927300</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Métricas finais de treino para FOLD 4:\n",
            "Loss: 0.4164\n",
            "Tempo total: 241.65s\n",
            "🧪 Avaliação no conjunto de teste/validação do FOLD 4...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados do FOLD 4: {'eval_loss': 0.3729526400566101, 'eval_accuracy': 0.9412, 'eval_auc': 0.9273, 'eval_f1_invasao': 0.9375, 'eval_f1_violencia': 0.9444, 'eval_runtime': 0.3121, 'eval_samples_per_second': 108.93, 'eval_steps_per_second': 16.019, 'epoch': 30.0}\n",
            "\n",
            "==================== INICIANDO FOLD 5/5 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Número de parâmetros treináveis no FOLD 5: 592130\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='510' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [510/510 04:15, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Auc</th>\n",
              "      <th>F1 Invasao</th>\n",
              "      <th>F1 Violencia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.672600</td>\n",
              "      <td>0.652862</td>\n",
              "      <td>0.666700</td>\n",
              "      <td>0.739000</td>\n",
              "      <td>0.592600</td>\n",
              "      <td>0.717900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.621200</td>\n",
              "      <td>0.630253</td>\n",
              "      <td>0.818200</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.823500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.589900</td>\n",
              "      <td>0.609397</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.783100</td>\n",
              "      <td>0.733300</td>\n",
              "      <td>0.777800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.541100</td>\n",
              "      <td>0.591379</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.786800</td>\n",
              "      <td>0.774200</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.536800</td>\n",
              "      <td>0.576494</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.801500</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.787900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.503200</td>\n",
              "      <td>0.560769</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.808800</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.450400</td>\n",
              "      <td>0.546799</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.808800</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.471300</td>\n",
              "      <td>0.533013</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.816200</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.480100</td>\n",
              "      <td>0.520889</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.827200</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.450700</td>\n",
              "      <td>0.516971</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.838200</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.417800</td>\n",
              "      <td>0.505273</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.841900</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.395000</td>\n",
              "      <td>0.494318</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.845600</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.438900</td>\n",
              "      <td>0.485962</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.852900</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.436000</td>\n",
              "      <td>0.478826</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.386000</td>\n",
              "      <td>0.468234</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.375400</td>\n",
              "      <td>0.462312</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.350100</td>\n",
              "      <td>0.455942</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.787900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.331000</td>\n",
              "      <td>0.451364</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.882400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.332200</td>\n",
              "      <td>0.447278</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.889700</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.331600</td>\n",
              "      <td>0.446943</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.889700</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.318500</td>\n",
              "      <td>0.444033</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.339200</td>\n",
              "      <td>0.441798</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.324800</td>\n",
              "      <td>0.438809</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.436597</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.313600</td>\n",
              "      <td>0.435051</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.347500</td>\n",
              "      <td>0.433169</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.323200</td>\n",
              "      <td>0.430739</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.304100</td>\n",
              "      <td>0.430523</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.315600</td>\n",
              "      <td>0.430360</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.374800</td>\n",
              "      <td>0.430393</td>\n",
              "      <td>0.757600</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.764700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Métricas finais de treino para FOLD 5:\n",
            "Loss: 0.4141\n",
            "Tempo total: 255.82s\n",
            "🧪 Avaliação no conjunto de teste/validação do FOLD 5...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados do FOLD 5: {'eval_loss': 0.430359810590744, 'eval_accuracy': 0.7576, 'eval_auc': 0.8934, 'eval_f1_invasao': 0.75, 'eval_f1_violencia': 0.7647, 'eval_runtime': 0.3046, 'eval_samples_per_second': 108.337, 'eval_steps_per_second': 16.415, 'epoch': 30.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. RESULTADOS FINAIS DA VALIDAÇÃO CRUZADA**"
      ],
      "metadata": {
        "id": "fFSsNAWvaQQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n==================== RESULTADOS FINAIS K-FOLD CROSS-VALIDATION ====================\")\n",
        "\n",
        "if all_fold_test_metrics:\n",
        "    # Coletar todas as métricas de todos os folds\n",
        "    df_metrics = pd.DataFrame(all_fold_test_metrics)\n",
        "\n",
        "    # Calcular médias e desvios padrão\n",
        "    avg_metrics = df_metrics.mean(numeric_only=True).round(4).to_dict()\n",
        "    std_metrics = df_metrics.std(numeric_only=True).round(4).to_dict()\n",
        "\n",
        "    print(\"Métricas Médias (e Desvio Padrão) em todos os Folds:\")\n",
        "    for metric, avg_value in avg_metrics.items():\n",
        "        # Excluir métricas de tempo/desempenho por segundo da apresentação principal se desejar\n",
        "        if 'runtime' not in metric and 'samples_per_second' not in metric and 'steps_per_second' not in metric:\n",
        "            print(f\"{metric}: {avg_value} (± {std_metrics.get(metric, 0)})\")\n",
        "else:\n",
        "    print(\"Nenhum resultado de fold foi coletado. Ocorreram erros em todos os folds.\")\n",
        "\n",
        "print(\"\\n✅ K-Fold Cross-Validation Concluído.\")"
      ],
      "metadata": {
        "id": "0a5gLzc6aQuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d238aba3-60e8-419e-bead-1053421cf062"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==================== RESULTADOS FINAIS K-FOLD CROSS-VALIDATION ====================\n",
            "Métricas Médias (e Desvio Padrão) em todos os Folds:\n",
            "eval_loss: 0.4136 (± 0.0528)\n",
            "eval_accuracy: 0.8221 (± 0.0764)\n",
            "eval_auc: 0.9012 (± 0.0365)\n",
            "eval_f1_invasao: 0.8128 (± 0.0786)\n",
            "eval_f1_violencia: 0.8304 (± 0.0745)\n",
            "epoch: 29.0 (± 2.2361)\n",
            "\n",
            "✅ K-Fold Cross-Validation Concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. SALVAR MODELO (Considerações Pós K-Fold)**"
      ],
      "metadata": {
        "id": "1SLL_XiZaSle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nConsiderações sobre o salvamento do modelo pós K-Fold:\")\n",
        "print(\"A Validação Cruzada fornece uma estimativa robusta da performance.\")\n",
        "print(\"Para ter um modelo final para deployment, você pode:\")\n",
        "print(\"1. Re-treinar o modelo uma última vez com o dataset COMPLETO (todos os 150 registros).\")\n",
        "print(\"2. Selecionar o modelo de um dos folds que obteve o melhor desempenho na sua respectiva validação.\")\n",
        "\n",
        "# Exemplo de como re-treinar no dataset completo (se necessário para deployment)\n",
        "# model_final = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     model_path, num_labels=2, id2label=id2label, label2id=label2id\n",
        "# )\n",
        "# for param in model_final.bert.parameters():\n",
        "#     param.requires_grad = False\n",
        "# for param in model_final.classifier.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# full_dataset_encodings = tokenize_function({\"texto\": df['texto'].tolist()})\n",
        "# full_dataset_labels = df['classe'].map(label2id).tolist()\n",
        "# full_custom_dataset = CustomDataset(pd.DataFrame(full_dataset_encodings), pd.Series(full_dataset_labels))\n",
        "\n",
        "# trainer_final = Trainer(\n",
        "#     model=model_final,\n",
        "#     args=TrainingArguments(\n",
        "#         output_dir=\"./bertimbau-final-model\",\n",
        "#         num_train_epochs=training_args.num_train_epochs, # Pode ser ajustado após ver os resultados da CV\n",
        "#         learning_rate=training_args.learning_rate,\n",
        "#         per_device_train_batch_size=training_args.per_device_train_batch_size,\n",
        "#         save_strategy=\"epoch\",\n",
        "#         load_best_model_at_end=True,\n",
        "#         metric_for_best_model=\"loss\", # Ou outra métrica se houver eval_dataset\n",
        "#         greater_is_better=False,\n",
        "#         logging_steps=training_args.logging_steps,\n",
        "#         fp16=training_args.fp16,\n",
        "#         seed=training_args.seed,\n",
        "#         report_to=\"none\"\n",
        "#     ),\n",
        "#     train_dataset=full_custom_dataset,\n",
        "#     data_collator=data_collator\n",
        "# )\n",
        "# print(\"\\nIniciando re-treinamento no dataset completo para modelo final...\")\n",
        "# trainer_final.train()\n",
        "# trainer_final.save_model(\"/content/drive/MyDrive/Pesquisa 2025/modelo_bertimbau_final_cv_retrained\")\n",
        "# tokenizer.save_pretrained(\"/content/drive/MyDrive/Pesquisa 2025/modelo_bertimbau_final_cv_retrained\")\n",
        "# print(\"Modelo final salvo no Google Drive após re-treinamento no dataset completo!\")"
      ],
      "metadata": {
        "id": "PbEnj8O5aUmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63c9f5a-de1f-4b9b-c048-75699e65e780"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Considerações sobre o salvamento do modelo pós K-Fold:\n",
            "A Validação Cruzada fornece uma estimativa robusta da performance.\n",
            "Para ter um modelo final para deployment, você pode:\n",
            "1. Re-treinar o modelo uma última vez com o dataset COMPLETO (todos os 150 registros).\n",
            "2. Selecionar o modelo de um dos folds que obteve o melhor desempenho na sua respectiva validação.\n"
          ]
        }
      ]
    }
  ]
}