{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kkt3yw00IoE"
      },
      "source": [
        "@inproceedings{souza2020bertimbau,\n",
        "  author    = {F{\\'a}bio Souza and\n",
        "               Rodrigo Nogueira and\n",
        "               Roberto Lotufo},\n",
        "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
        "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
        "  year      = {2020}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BkkHsJY5z-5",
        "outputId": "37f0e0f8-7d8e-45f9-fdd0-540869f06035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbstripout in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from nbstripout) (5.10.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (4.25.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (5.8.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.26.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->nbstripout) (4.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->nbstripout) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nbstripout\n",
        "!nbstripout /content/drive/MyDrive/Pesquisa-2025/fine-tuning_Bertimbau.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feHRGfqdONsa"
      },
      "outputs": [],
      "source": [
        "!pip install transformers evaluate accelerate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLm55Na1gv-t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define um modelo simples\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(10, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "# Cria o modelo\n",
        "model = Model()\n",
        "\n",
        "# Define o dispositivo (GPU se dispon√≠vel)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Move o modelo para a GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Exemplo de uso\n",
        "dados = torch.randn(32, 10).to(device)  # Move os dados tamb√©m\n",
        "saida = model(dados)\n",
        "print(saida.shape)  # Deveria mostrar torch.Size([32, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkRUcLU1PHke"
      },
      "source": [
        "# **# 1. Carregar Dataset Processado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O3x60E3PHtT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset as TorchDataset\n",
        "# Carregar dataset j√° processado\n",
        "file_path = '/content/drive/MyDrive/Pesquisa 2025/dataset/denuncias_balanceadas.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Dividir em treino (70%), valida√ß√£o (15%) e teste (15%)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['classe'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['classe'], random_state=42)\n",
        "\n",
        "print(f\"Treino: {len(train_df)}, Valida√ß√£o: {len(val_df)}, Teste: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE3WAEgOPYHa"
      },
      "source": [
        "# **2. Tokeniza√ß√£o com BERTimbau**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sql7RSXMPd1g"
      },
      "outputs": [],
      "source": [
        "model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Fun√ß√£o de tokeniza√ß√£o\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(batch[\"texto\"], truncation=True,max_length=512, padding='max_length',return_tensors='pt')\n",
        "\n",
        "# Tokenizar os DataFrames diretamente (sem usar datasets.Dataset)\n",
        "train_encodings = tokenize_function(train_df.to_dict('list'))\n",
        "val_encodings = tokenize_function(val_df.to_dict('list'))\n",
        "test_encodings = tokenize_function(test_df.to_dict('list'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0mzN3JPP-s4"
      },
      "source": [
        "# **3. Configurar o Modelo com congelamento ajustado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkQIL_4sYTMq"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_path,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Congelar camadas do BERT (exceto pooler e classificador)\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' not in name and 'pooler' not in name:\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S-nz2HQZ0CA"
      },
      "source": [
        "# **4. M√âTRICAS AVAN√áADAS (Acur√°cia + AUC-ROC)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-gctrIbZ1Pv"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "auc_score = evaluate.load(\"roc_auc\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    try:\n",
        "        predictions, labels = eval_pred\n",
        "\n",
        "        # Converter logits em probabilidades\n",
        "        probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
        "        positive_class_probs = probabilities[:, 1]\n",
        "\n",
        "        # Calcular m√©tricas b√°sicas\n",
        "        preds = np.argmax(predictions, axis=1)\n",
        "        acc = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "        auc = auc_score.compute(prediction_scores=positive_class_probs, references=labels)[\"roc_auc\"]\n",
        "\n",
        "        # Calcular F1 para cada classe com tratamento de erro\n",
        "        f1_results = f1_metric.compute(\n",
        "            predictions=preds,\n",
        "            references=labels,\n",
        "            average=None,\n",
        "            labels=[label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": round(acc, 4),\n",
        "            \"auc\": round(auc, 4),\n",
        "            \"f1_invasao\": round(f1_results[\"f1\"][0], 4),\n",
        "            \"f1_violencia\": round(f1_results[\"f1\"][1], 4)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no c√°lculo de m√©tricas: {str(e)}\")\n",
        "        return {\"accuracy\": 0.0, \"auc\": 0.0, \"f1_invasao\": 0.0, \"f1_violencia\": 0.0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fmwNJA-Z5DK"
      },
      "source": [
        "# **TREINAMENTO COM EARLY STOPPING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu5KpVfWaGA3"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bertimbau-denuncias\",\n",
        "\n",
        "    # Configura√ß√µes b√°sicas\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    #num_train_epochs=10,\n",
        "    num_train_epochs=3,\n",
        "\n",
        "    # Estrat√©gias (nomenclatura correta para v4.53.2)\n",
        "    eval_strategy=\"epoch\",          # evaluation_strategy n√£o existe nesta vers√£o\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    # Configura√ß√µes de avalia√ß√£o\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    # Logging\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    disable_tqdm=False,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # Otimiza√ß√µes\n",
        "    fp16=True,\n",
        "    seed=42,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    # Par√¢metros espec√≠ficos da v4.53.2\n",
        "    remove_unused_columns=True,\n",
        "    label_names=[\"labels\"]\n",
        ")\n",
        "\n",
        "# Data Collator para padding din√¢mico\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Custom Dataset (para trabalhar com DataFrames)\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "class CustomDataset(TorchDataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels.iloc[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Criar datasets\n",
        "train_dataset = CustomDataset(train_encodings, train_df['classe'].map(label2id))\n",
        "val_dataset = CustomDataset(val_encodings, val_df['classe'].map(label2id))\n",
        "test_dataset = CustomDataset(test_encodings, test_df['classe'].map(label2id))\n",
        "\n",
        "# Trainer\n",
        "from transformers import Trainer, EarlyStoppingCallback\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Para early stopping\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "# Treinar!\n",
        "#print(\"\\nIniciando treinamento...\")\n",
        "#trainer.train()\n",
        "\n",
        "try:\n",
        "    print(\"\\nüöÄ Iniciando treinamento...\")\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # Salvar m√©tricas de treino\n",
        "    metrics = train_result.metrics\n",
        "    print(\"\\nüìä M√©tricas finais de treino:\")\n",
        "    print(f\"Loss: {metrics['train_loss']:.4f}\")\n",
        "    print(f\"Tempo total: {metrics['train_runtime']:.2f}s\")\n",
        "\n",
        "    # Avaliar no conjunto de teste\n",
        "    print(\"\\nüß™ Avalia√ß√£o no conjunto de teste...\")\n",
        "    test_metrics = trainer.evaluate(CustomDataset(test_encodings, test_df['classe'].map(label2id)))\n",
        "    print(f\"Acur√°cia: {test_metrics['eval_accuracy']:.4f}\")\n",
        "    print(f\"AUC: {test_metrics['eval_auc']:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Erro durante o treinamento: {str(e)}\")\n",
        "    # Adicione aqui qualquer l√≥gica adicional de tratamento de erro\n",
        "finally:\n",
        "    # Salvamento seguro do modelo\n",
        "    try:\n",
        "        trainer.save_model(\"./modelo_final\")\n",
        "        print(\"\\nüíæ Modelo salvo com sucesso!\")\n",
        "    except:\n",
        "        print(\"\\n‚ö†Ô∏è Erro ao salvar o modelo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFSsNAWvaQQs"
      },
      "source": [
        "# **6. AVALIA√á√ÉO FINAL NO TESTE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a5gLzc6aQuU"
      },
      "outputs": [],
      "source": [
        "print(\"\\nAvalia√ß√£o no conjunto de teste:\")\n",
        "test_results = trainer.predict(test_dataset)\n",
        "print(test_results.metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SLL_XiZaSle"
      },
      "source": [
        "# **7. SALVAR MODELO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEnj8O5aUmx"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Pesquisa 2025/modelo_bertimbau_final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Pesquisa 2025/modelo_bertimbau_final\")\n",
        "print(\"Modelo salvo no Google Drive!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}